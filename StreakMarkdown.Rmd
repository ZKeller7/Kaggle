---
title: "| Predicting Sports Outcomes \n| Using ESPN Contest Data to Build a Predictive
  Model\n"
author: "Zachary Keller"
date: "March 14, 2016"
output: pdf_document
sansfont: Calibri Light
---
[Link to GitHub Code](https://github.com/ZKeller7/ML)

## Introduction and Overview

ESPN runs a promotional contest for its online users called Streak for the Cash. In the contest, users are presented with matchups from a variety of sports and asked to predict the correct outcome. On average there are 15-25 matchups to choose from on any particular day. For example, a matchup from March 14, 2016 was: 

"Who will WIN this matchup? Detroit Pistons or Washington Wizards?"

Another example from March 12:

"What will be the GAME RESULT?: Georgia: Wins or Single Digit Loss or Kentucky: Wins By Double Digits?"

The contest is subject to the following conditions:  
1. There are a variety of types of matchups presented from all major sports and leagues, with the winner of the contest being the user who puts together the longest "streak" of correct picks in the presented matchups in each calendar month.  
2. Users may select any matchup that they wish, and are not compelled to make a selection for any particular matchup.  
3. Users select only one matchup at a time, but may select as many non-concurrent matchups as they wish each day.   
4. If a user picks a matchup incorrectly, their streak is reset to zero. They may start their streak again at any time.  
5. The prize for the longest streak is typically $30,000, but may reach as high as $1,000,000.  
6. Crucially, the matchups display the percentage of participating users who have selected each side. In the first example above, it was shown that 39.7% of users who made a selection picked Detroit, with 60.3% picking Washington.  
7. After the sporting event in question has begun, no selection may be made. 


From these conditions, we can draw a few inferences. First, since the penalty for picking incorrectly is so high, combined with the fact that users are allowed to pick any matchup they feel most comfortable with, we can infer that *in general* users will make selections in matchups that they feel particularly knowledgeable about, or have insight on. This leads to the question under consideration - can these mildly informed crowd sourced predictions lead to accurate predictions? Can we build a model using the pick data to predict sporting event outcomes?

## Collecting Data

First we need to collect data. Using the XML package we can scrape data tables from ESPN's website. Each table contains the entire list of matchups from any particular day, including the teams playing, the sport, the vote total for each side, and a few other features. We can use the following code to scrape:

```{r, echo = FALSE, warning= FALSE, message=FALSE}
library(XML)
library(sampling)
library(dplyr)
library(caret)
library(ggplot2)
library(scales)

MyColNames <- c("Time", "Side_One", "Sport", "Score_One", "Side_Two", "Vote_One", "Score_Two", "Status", "Vote_Two", "Warmth", "ID")
makevec <- function(x) {
  r <- 1
  q <- vector('numeric')
  T <- ifelse(is.null(ncol(x)), 1, ncol(x))
  P <- ifelse(is.null(nrow(x)), 1, nrow(x))
  for (y in 1:T) {
    for (i in 1:P) {
      if (!is.na(x[i,y])) {
        if (x[i,y] != "") {
          q[r] <- x[i,y]
          r <- r + 1
        }
      }
    }
  }
  q <- as.data.frame(t(q))
  q[r] <- paste(sample(0:9, 10, replace = TRUE), collapse = "")
  return(q)
}
```

```{r, cache = TRUE}
baseURL <- "http://streak.espn.go.com/en/entry?date=201501"
for (URLAdd in 1:500) {
  LoopedURL <- paste0(baseURL, URLAdd)
  kappa <- readHTMLTable(doc = LoopedURL, as.data.frame = TRUE, stringsAsFactors = FALSE)
  kappa <- kappa[-length(kappa)]
  if (length(kappa) < 2) {
    next
  } else {
    InterimStreakData <- lapply(kappa, makevec)
    InterimStreakData <- do.call(rbind, Filter(function(x) length(x) == 11, InterimStreakData))
    ifelse(URLAdd == 1,
           BigStreakData <- InterimStreakData,
           BigStreakData <- data.frame(rbind(InterimStreakData, BigStreakData)))
  }
}
```

Fortunately, ESPN allows us to start from January 1st 2015 and increment the number of days since then to get the table of matchups for each date from that date to the present. Executing the code above gives us 10046 unique observations - note that the function makevec in the above snippet is a custom function to accurately transpose the raw data, full code is on the linked github. 

Now we've returned all of this data - remember with each example recording a particular matchup - with the following features.
```{r, echo = FALSE}
names(BigStreakData) <- MyColNames
FinalStreakData <- unique(BigStreakData[,1:10])
sdata <- FinalStreakData[, c(-1,-8)]
```
```{r}
str(sdata)
```
```{r, echo = FALSE}
sdata$Sport <- factor(sdata$Sport)
sdata$Vote_One <- gsub("%", "", sdata$Vote_One)
sdata$Vote_Two <- gsub("%", "", sdata$Vote_Two)
sdata$Score_One <- as.numeric(sdata$Score_One)
sdata$Score_Two <- as.numeric(sdata$Score_Two)
sdata$Vote_One <- as.numeric(sdata$Vote_One)
sdata$Vote_Two <- as.numeric(sdata$Vote_Two)
```

Now we need to do some serious data cleaning, but that's not particularly interesting so lets skip ahead until we are at a point where we can create some new features. Arbitrarily using the first team listed in the table, referred to as "Team 1" or "T1", as a reference group, we will create a feature that tells us if Team 1 has won or not won, a feature that indicates if the majority of users voted for Team 1, and create a target feature that tells us if the majority was correct. Finally, we will use a table to display the proportion of examples in which users correctly predicted the outcome. 

```{r}
sdata$T1Wins <- ifelse(sdata$Score_One > sdata$Score_Two, TRUE, FALSE)
sdata$VotedT1 <- ifelse(sdata$Vote_One >= sdata$Vote_Two, TRUE, FALSE)
sdata$VotersRight <- ifelse(sdata$T1Wins == sdata$VotedT1, TRUE, FALSE)
prop.table(table(sdata$VotersRight))
```

Here we can answer the first part of our question. We can see that ESPN users clearly should not attempt to take their talents to Las Vegas. The likelihood that the majority vote on any particular matchup is the correct choice is only 51.6%, barely better than flipping a coin! Still, there may yet be fruit in the search for the answer to our second question - can we build a model that will tell us, based on features that we have before the matchup is locked, if it is likely to be predicted correctly?

```{r, echo = FALSE}
binner <- function(x) {
  if (x < 15) {
    "One-Sided Against"} 
  else if (x < 35 && x >= 15) {
    "Moderate Against"} 
  else if (x < 50 && x >= 35) {
    "Mild Against"}
  else if (x < 65 && x >= 50) {
    "Mild For"} 
  else if (x < 85 && x >= 65) {
    "Moderate For"}
  else if (x >= 85) {
    "One-Sided For"}
}
sdata$StrengthofVoteforT1 <- sapply(sdata$Vote_One, binner)
#catching type of bet
sdata$Type <- ifelse(grepl("@", substr(sdata$Side_One,1,3), ignore.case = TRUE) == TRUE, "Heads Up",
                     ifelse(grepl("Wins By|Draw", sdata$Side_One) == TRUE | grepl("Wins By|Draw", sdata$Side_Two) == TRUE, "Wins By", 
                            ifelse(grepl("Both|Either", sdata$Side_One) == TRUE | grepl("Both|Either", sdata$Side_Two) == TRUE, "Parley", "Prop")))

#catching home field
sdata$T1Home <- ifelse(grepl("@", substr(sdata$Side_One,1,3), ignore.case = TRUE) == TRUE, TRUE, FALSE)

#catching records
#Side one

RecordPull <- function(x) {
  x <- gsub("[\\(\\)]", "", regmatches(x, gregexpr("\\(.*?\\)", x))[[1]])
  x <- as.vector(na.omit(as.numeric(unlist(strsplit(unlist(x), "[^0-9]+")))))
  WP <- vector('numeric')
  if (length(x) == 0) {return(NA)}
  #calucate winning percentage
  if (length(x) == 2) {
    WP <- x[1]/sum(x)
  } else if (length(x) == 3) {
    WP <- (x[1] + .5 * x[3])/sum(x)
  }
  if (length(x) == 1) {
    return(NA)
  }
  return(WP)
}

sdata$Side_OneRecord <- sapply(sdata$Side_One, RecordPull)
sdata$side_TwoRecord <- sapply(sdata$Side_Two, RecordPull)

#bin record disparity
recbin <- function(x,y) {
  if (is.na(x) | is.na(y) == TRUE) {return(NA)}
  x <- x + .01
  y <- y + .01
  if ((x/y) >= 1.25) {
    "Strong T1 Advantage"
  }
  else if ((x/y) >= 1 && (x/y) < 1.25) {
    "Moderate T1 Advantage"
  }
  else if ((x/y) >= .75 && (x/y) < 1) {
    "Moderate T1 Disadvantage"
  } 
  else if ((x/y) < .75) {
    "Strong T1 Disadvantage"
  }
}

sdata$RecordDisparity <- mapply(recbin, sdata$Side_OneRecord, sdata$side_TwoRecord)
sdata[is.na(sdata)] <- "None"
mod.data <- sdata[which(sdata$Sport %in% names(which(table(sdata$Sport) > 100))),]
mod.data$Sport <- factor(mod.data$Sport)
mod.data$VotersRight <- ifelse(mod.data$VotersRight == T, 1, 0)
mod.data <- mod.data[order(mod.data$Sport),]
rownames(mod.data) <- c(1:nrow(mod.data))
mod.data <- mod.data[,-c(1,3,4,6,9)]
mod.data[,7] <- factor(mod.data[,7])
mod.data[,8] <- factor(mod.data[,8])
mod.data[,12] <- factor(mod.data[,12])
mod.data[which(mod.data[,10] == "None"),10] <- .5
mod.data[which(mod.data[,11] == "None"),11] <- .5
mod.data[,10:11] <- sapply(mod.data[,10:11], as.numeric)
mod.data <- cbind(mod.data[,c(1:5,7:12)], mod.data[,6])
colnames(mod.data)[12] <- "VotersRight"
```
## Cleaned Data

After a little more cleaning and feature creation, we arrive at a data set with the following structure. Each of the features in the following set, less the target feature, is available publically before the sporting event begins.

```{r}
str(mod.data)
```
The features are described as:  
1. Sport - the sport featured in the matchup.  
2. Vote One - the vote for Team 1.  
3. Vote Two - the vote for Team 2.  
4. Warmth - an ESPN metric that indicates the number and speed at which people are picking the matchup.  
5. Voted T1 - a boolean variable indicating if the majority of participants voted for Team 1.  
6. Strength of Vote for T1 - a categorical feature indicating the gap between the vote for Team 1 compared to Team 2.  
7. Type - the type of matchup, e.g. Prop, Heads Up, Parley, etc.  
8. T1 Home - boolean feature indicating of Team 1 is the home team.  
9. Side One Record - the win:loss record for Team 1, if available.  
10. Side Two Record - the win:loss record for Team 2, if available. 
11. Record Disparity - the disparity between the win:loss records of the two teams, if available.  
12. Voters Right - our target feature, a binary variable indicating if the majority vote was correct.  

## Building a Model

Finally we can start building our model. Since we have several categorical features and our feature space is fairly small and therefore will grow smaller trees, I am electing to use a boosted tree approach with XGBoost. Though the following implementation is, strictly speaking, one of classification, if we build a model that can accurately classify new examples into the classes "Majority Vote Incorrect" and "Majority Vote Correct", then it is producing a result that is easily decisionable and making defacto predictions. That is, if the model can accurately tell us when the majority vote is correct or incorrect, by extension it is telling us the result of the sporting event. An alternate choice would be to use something like logit regression, but due to the high amount of categorical features that may be a cumbersome approach.  
  
  Our first step is to split the data into training and validation sets with their corresponding labels.
```{r}
set.seed(7)
q <- sample(nrow(mod.data), nrow(mod.data) * .8)
Train <- mod.data[q,]
Validate <- mod.data[-q,]

ValLabs <- Validate$VotersRight
Validate <- data.matrix(Validate[,-12])
ValLabs <- factor(ValLabs, levels = c(0,1))

TrainLabs <- Train$VotersRight
Train <- data.matrix(Train[,-12])
TrainLabs <- factor(TrainLabs, levels = c(0,1))
```
```{r, echo = FALSE}
levels(TrainLabs) <- c("o", "l")
```

Before we build, we should try to parameterize our model, specifically the learning rate and depth of the trees. 

```{r, message=FALSE, error=FALSE, warning=FALSE}
ControlGrid <- expand.grid(nrounds   = 250, #tuning this later
                           eta       = c(.001,.005,.01),
                           max_depth = c(2,3,4))

Control <- trainControl(method          = "cv",
                        number          = 10,
                        returnData      = FALSE,
                        returnResamp    = "all",
                        classProbs      = TRUE,
                        summaryFunction = twoClassSummary)

TrainHyper = train(x         = Train,
                   y         = TrainLabs,
                   trControl = Control,
                   tuneGrid  = ControlGrid,
                   method    = "xgbTree")
```

After passing the control parameters and control grid to the train function, we can plot the AUC of the tested combinations.

```{r fig.width=7.5, fig.height=5.5,echo=FALSE, fig.align="center"}
ggplot(TrainHyper$results, aes(x = as.factor(eta), y = max_depth, size = ROC, color = ROC)) + 
  geom_point() + 
  theme_bw() + 
  scale_size_continuous(guide = "none") +
  labs(x="ETA",y="Max Depth") 
```
```{r}
TrainHyper$results[1:9]
```

We can see that the best combination seems to be a maximum depth of 3 and a learning rate of .005. Next we will tune the number of trees to be grown with the xgb.cv function. 

```{r, echo = FALSE}
levels(TrainLabs) <- c("0", "1")
```
```{r}
ParamSet <- list(objective           = "binary:logistic", 
                 booster             = "gbtree",
                 eval_metric         = "auc",
                 eta                 = .005,
                 max_depth           = 3,
                 nthread             = 8)

crossval <- xgb.cv(params            = ParamSet,
                   data              = Train,
                   nrounds           = 500,
                   nfold             = 10,
                   label             = as.integer(sapply(TrainLabs, as.character)),
                   metrics           = "auc",
                   stratified        = T,
                   verbose           = F,
                   maximize          = TRUE)
trees <- which(crossval$test.auc.mean == max(crossval$test.auc.mean))
print(trees)
```
```{r fig.width=7, fig.height=5,echo=FALSE, fig.align="center"}
ggplot(data = crossval, aes(x = seq_along(crossval$test.auc.mean), y = crossval$test.auc.mean)) +
geom_line() +
labs(x="Trees",y="AUC")   
```
  
  We can see from the plot and the output from the cross validation function that the optimal point seems to be at `r trees` trees. We will use the set of hyperparameters that we've found to build our model and test it against the validation set. 

Finally we are ready to build the model:
```{r, echo = FALSE}
levels(TrainLabs) <- c(0,1)
```
```{r}
boost.mod <- xgboost(params              = ParamSet, 
                     data                = Train,
                     label               = as.numeric(sapply(TrainLabs, as.character)),
                     nrounds             = trees, #from cross validation
                     verbose             = F,
                     maximize            = TRUE,
                     stratified          = T)

Predictions <- predict(boost.mod, Validate)
BinaryPreds <- round(Predictions, digits = 0)

CM <- confusionMatrix(BinaryPreds, ValLabs)
CM
```

## Model Review

From reviewing the confusion matrix we see a few things. One, if we apply this model to make choices in the contest, we will predict the outcomes with approximately `r percent(round(CM$overall[1], digits = 5))` accurary - an improvement of about 10%, much better than following the majority. Professional sports gambling doesn't seem to lend itself to rigorous academic publications, but from a quick google search it seems that a professional sports bettor is considered very good if they can acheive an accuracy over 55% - our model judged by that benchmark is excellent.  
  
  We can further see that we are quite good at classifying the examples where the voters are correct with a specifiity of `r percent(round(CM$byClass[2], digits = 5))`, but not as good at classifying examples where they were wrong, with a sensitivity of only `r percent(round(CM$byClass[1], digits = 5))`, not much better than random guessing. As mentioned before, although we've built a classifier, we can use it for predictions by simply applying new examples to the model and using its classifications to make a prediction. It remains to be seen if the model will generalize to unseen data, perhaps a revisit in a few months will help to pinpoint potential problems with the model.  
    
  Still, we can definitely say that the answer to our original question - can we build a model using the data available on ESPN's Streak for the Cash to predict sporting event outcomes before those events begin - is a YES. Our model performs better than ESPN users as a group and better than most professional sports bettors. In that respect, we have succeeded. 

##Potential Improvement Points

Despite our success, there is always room for improvement. The model could perhaps be improved if it were ensembled with more traditional sports prediction methods, such as building a model based on team and individual statistics and then ensembling it with the Streak for the Cash model. It might also be improved by collecting the comments section from each matchup - there users can leave messages on their thoughts about the game. Builing out a sentiment analysis from those comments may add some predictive value.   
  
  As previously mentioned, using logit regression may be an approach that is worth investigating. That method would produce a true predictive model - though that is not necessarily an advantage given the excellent results we have already obtained with our classifier. 






